{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df19a9e-27a8-4869-adc6-c6b9e76f48b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"../../\")\n",
    "\n",
    "from autogluon.vision import ImagePredictor, ImageDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from cleanlab.count import get_confident_thresholds\n",
    "from cleanlab.outlier import OutOfDistribution\n",
    "from cleanlab.internal.label_quality_utils import get_normalized_entropy\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7895ad-11e9-4454-90cb-c7c33dae84d9",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2db9dfd-303c-43c2-9439-05a994dee909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Getting data for cifar-10\n",
      "--------------------------\n",
      "Getting data for cifar-100\n",
      "CPU times: user 225 ms, sys: 25.1 ms, total: 250 ms\n",
      "Wall time: 248 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = \"swin_base_patch4_window7_224\" # uses Torch backend\n",
    "# model = \"resnet50_v1\" # uses MXNET backend\n",
    "\n",
    "data_model_dict = {\n",
    "    \"cifar-10\": {\"data_path\": \"/datasets/uly/ood-data/cifar10_png/\"},\n",
    "    \"cifar-100\": {\"data_path\": \"/datasets/uly/ood-data/cifar100_png/\"},\n",
    "#     \"mnist\": {\"data_path\": \"/datasets/uly/ood-data/mnist_png/\"},\n",
    "#     \"fashion-mnist\": {\"data_path\": \"/datasets/uly/ood-data/fashion_mnist_png/\"}\n",
    "}\n",
    "\n",
    "# Get data, model, and pre-trained features\n",
    "for dataset in data_model_dict.keys():\n",
    "    \n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Getting data for {dataset}\")\n",
    "    \n",
    "    # Get path to data\n",
    "    data_path = data_model_dict[dataset][\"data_path\"]\n",
    "    \n",
    "    # Get train and test data\n",
    "    data_model_dict[dataset][\"train_data\"], _, data_model_dict[dataset][\"test_data\"] = \\\n",
    "        ImageDataset.from_folders(root=data_path)\n",
    "    \n",
    "    # Get path to saved model\n",
    "    data_model_dict[dataset][\"model\"] = f\"./models040_balanced/{model}_{dataset}.ag\"\n",
    "    \n",
    "    # Get path to saved xval results\n",
    "    data_model_dict[dataset][\"xval\"] = f\"./{dataset}_{model}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cace151-c2b3-476d-9c4b-0a49f8ca7acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0446d-8978-449f-b8c5-0cbdd97d46be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d426f01d-d145-4d18-a265-0b8c866114db",
   "metadata": {},
   "source": [
    "## Evaluate models on test data as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b533948d-5031-4de5-bb1c-0b0ce9eb259a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Dataset: cifar-10\n",
      "  Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uly/virtual/test_ood_040/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 0] validation: top1=0.977800 top5=0.999800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation: {'loss': 0.1624536116063595, 'top1': 0.9778, 'top5': 0.9998}\n",
      "----------------------------------\n",
      "Dataset: cifar-100\n",
      "  Loading model...\n",
      "  Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1] validation: top1=0.843100 top5=0.976900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation: {'loss': 0.6250032405972481, 'top1': 0.8431, 'top5': 0.9769}\n",
      "CPU times: user 2min 34s, sys: 12.8 s, total: 2min 46s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "accuracy_result_list = []\n",
    "\n",
    "for key, data in data_model_dict.items():\n",
    "    dataset = key\n",
    "\n",
    "    model_path = data[\"model\"]\n",
    "    test_dataset = data[\"test_data\"]\n",
    "    \n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    \n",
    "    # load model\n",
    "    print(\"  Loading model...\")\n",
    "    predictor_loaded = ImagePredictor.load(model_path)\n",
    "    \n",
    "    # evaluating model on test data\n",
    "    print(\"  Evaluating model...\")\n",
    "    eval_ = predictor_loaded.evaluate(test_dataset)\n",
    "    print(f\"    Evaluation: {eval_}\")\n",
    "    \n",
    "    accuracy_result = {\n",
    "        \"dataset\": dataset,\n",
    "        \"top1\": eval_[\"top1\"]\n",
    "    }\n",
    "    \n",
    "    accuracy_result_list.append(accuracy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99027a0-8bac-476b-972d-e7a94eaae158",
   "metadata": {},
   "source": [
    "## Evaluate OOD Scores on TEST data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2932f76-a8c7-4009-bff1-f672da79923a",
   "metadata": {},
   "source": [
    "## Save the pred_probs and features used for OOD scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948bea89-2597-4163-9f53-b93090c233b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "(in-distribution, out-of-distribution) dataset pair:  cifar-10 cifar-100\n",
      "  Loading model...\n",
      "  Generating predicted probabilities...\n",
      "-----------------------------------------------------\n",
      "(in-distribution, out-of-distribution) dataset pair:  cifar-100 cifar-10\n",
      "  Loading model...\n",
      "  Generating predicted probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uly/virtual/test_ood_040/lib/python3.8/site-packages/pandas/core/frame.py:3678: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self[col] = igetitem(value, i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 45s, sys: 22.7 s, total: 6min 8s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# pairs of (in-distribution, out-of-distribution) datasets to evaluate\n",
    "in_out_pairs = [\n",
    "    {\"in\": \"cifar-10\", \"out\": \"cifar-100\"},\n",
    "    {\"in\": \"cifar-100\", \"out\": \"cifar-10\"},\n",
    "#     {\"in\": \"mnist\", \"out\": \"fashion-mnist\"},\n",
    "#     {\"in\": \"fashion-mnist\", \"out\": \"mnist\"},\n",
    "]\n",
    "\n",
    "for in_out_pair in in_out_pairs:\n",
    "    \n",
    "    in_dataset, out_dataset = in_out_pair[\"in\"], in_out_pair[\"out\"]\n",
    "    \n",
    "    # path to model trained on in-distribution train dataset\n",
    "    in_model_path = data_model_dict[in_dataset][\"model\"]\n",
    "\n",
    "    # get in-distribution TRAIN dataset\n",
    "    in_train_dataset = data_model_dict[in_dataset][\"train_data\"]\n",
    "    in_train_dataset_class_labels = np.load(data_model_dict[in_dataset][\"xval\"] + \"/in_train_dataset_class_labels.npy\") # class labels for the in-distribution train dataset\n",
    "    assert (in_train_dataset.label.values == in_train_dataset_class_labels).all()\n",
    "    \n",
    "    # get TEST datasets used for evaluation\n",
    "    in_test_dataset = data_model_dict[in_dataset][\"test_data\"]\n",
    "    in_test_dataset_class_labels = in_test_dataset.label.values # class labels for the in-distribution test dataset\n",
    "\n",
    "    out_test_dataset = data_model_dict[out_dataset][\"test_data\"]\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"(in-distribution, out-of-distribution) dataset pair: \", in_dataset, out_dataset)\n",
    "    \n",
    "    # load model (trained on training set)\n",
    "    print(\"  Loading model...\")\n",
    "    in_predictor_loaded = ImagePredictor.load(in_model_path)\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    print(\"  Generating predicted probabilities...\")\n",
    "    in_train_pred_probs = np.load(data_model_dict[in_dataset][\"xval\"] + \"/in_train_dataset_pred_probs.npy\")\n",
    "    in_test_pred_probs = in_predictor_loaded.predict_proba(data=in_test_dataset, as_pandas=False)\n",
    "    out_test_pred_probs = in_predictor_loaded.predict_proba(data=out_test_dataset, as_pandas=False)\n",
    "    \n",
    "    # Save files here\n",
    "    out_folder = f\"./model_{model}_experiment_in_{in_dataset}_out_{out_dataset}/\"\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    \n",
    "    #### Uncomment below to save files\n",
    "\n",
    "    np.save(out_folder + \"in_train_pred_probs.npy\", in_train_pred_probs)\n",
    "    np.save(out_folder + \"in_test_pred_probs.npy\", in_test_pred_probs)\n",
    "    np.save(out_folder + \"out_test_pred_probs.npy\", out_test_pred_probs)\n",
    "    \n",
    "    np.save(out_folder + \"in_train_dataset_class_labels.npy\", in_train_dataset_class_labels)\n",
    "    np.save(out_folder + \"in_test_dataset_class_labels.npy\", in_test_dataset_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4978ade5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model_swin_base_patch4_window7_224_experiment_in_cifar-10_out_cifar-100/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47837b90-a36f-4c50-a966-33f09b2bb31a",
   "metadata": {},
   "source": [
    "## Run OOD scoring on loaded pred_probs and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf18e806-4a90-4662-88ed-bd092593a439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(p, q):\n",
    "    return -np.sum(p * np.log(q)) / q.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4298896e-4a98-4de8-93e9-26dc3e5fd2b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "(in-distribution, out-of-distribution) dataset pair:  cifar-10 cifar-100\n",
      "Fitting OOD estimator based on provided pred_probs ...\n",
      "Fitting OOD estimator based on provided pred_probs ...\n",
      "-----------------------------------------------------\n",
      "(in-distribution, out-of-distribution) dataset pair:  cifar-100 cifar-10\n",
      "Fitting OOD estimator based on provided pred_probs ...\n",
      "Fitting OOD estimator based on provided pred_probs ...\n",
      "CPU times: user 206 ms, sys: 42.1 ms, total: 248 ms\n",
      "Wall time: 248 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# pairs of (in-distribution, out-of-distribution) datasets to evaluate\n",
    "in_out_pairs = [\n",
    "    {\"in\": \"cifar-10\", \"out\": \"cifar-100\"},\n",
    "    {\"in\": \"cifar-100\", \"out\": \"cifar-10\"},\n",
    "#     {\"in\": \"mnist\", \"out\": \"fashion-mnist\"},\n",
    "#     {\"in\": \"fashion-mnist\", \"out\": \"mnist\"},\n",
    "]\n",
    "\n",
    "k_max = 110\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for in_out_pair in in_out_pairs:\n",
    "    \n",
    "    in_dataset, out_dataset = in_out_pair[\"in\"], in_out_pair[\"out\"]\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"(in-distribution, out-of-distribution) dataset pair: \", in_dataset, out_dataset)\n",
    "    \n",
    "    # Save files here\n",
    "    out_folder = f\"./model_{model}_experiment_in_{in_dataset}_out_{out_dataset}/\"\n",
    "    \n",
    "    # Load files\n",
    "    in_train_pred_probs = np.load(out_folder + \"in_train_pred_probs.npy\")\n",
    "    in_test_pred_probs = np.load(out_folder + \"in_test_pred_probs.npy\")\n",
    "    out_test_pred_probs = np.load(out_folder + \"out_test_pred_probs.npy\")\n",
    "    \n",
    "    in_train_dataset_class_labels = np.load(out_folder + \"in_train_dataset_class_labels.npy\")\n",
    "    in_test_dataset_class_labels = np.load(out_folder + \"in_test_dataset_class_labels.npy\")\n",
    "\n",
    "    # Combine pred_probs and features for TEST dataset\n",
    "    test_pred_probs = np.vstack([in_test_pred_probs, out_test_pred_probs])\n",
    "    \n",
    "    # Create OOD binary labels (1 = out-of-distribution)\n",
    "    in_labels = np.zeros(shape=len(in_test_pred_probs))\n",
    "    out_labels = np.ones(shape=len(out_test_pred_probs))\n",
    "    ood_mask = np.hstack([in_labels, out_labels]).astype(int) # OOD binary indicator\n",
    "\n",
    "    #### Compute nearest neighbors\n",
    "    \n",
    "    #### Get scores\n",
    "    \n",
    "    # Adjusted MSP & Entropy in confidence thresholds\n",
    "    class_confident_thresholds = get_confident_thresholds(in_train_dataset_class_labels, in_train_pred_probs, multi_label=False)\n",
    "    \n",
    "    # Train Entropy\n",
    "    in_train_entropy = get_normalized_entropy(in_train_pred_probs)\n",
    "    \n",
    "    # OOD object Entropy and MSP\n",
    "    OOD_msp = OutOfDistribution(params={\"method\": \"least_confidence\"})    \n",
    "    OOD_entropy = OutOfDistribution()\n",
    "    \n",
    "    OOD_msp.fit(pred_probs=in_train_pred_probs, labels=in_train_dataset_class_labels)    \n",
    "    OOD_entropy.fit(pred_probs=in_train_pred_probs, labels=in_train_dataset_class_labels)\n",
    "    \n",
    "    #### Get scores for test dataset\n",
    "    \n",
    "    # 1 - Max Pred Probs\n",
    "    test_one_minus_max_pred_prob = 1. - test_pred_probs.max(axis=1)\n",
    "\n",
    "    # Entropy\n",
    "    test_entropy = get_normalized_entropy(test_pred_probs)\n",
    "\n",
    "    # Adjust pred-probs for Adjusted MSP and Entropy\n",
    "    test_pred_probs_adj = test_pred_probs - class_confident_thresholds\n",
    "    test_pred_probs_adj += class_confident_thresholds.max()\n",
    "    test_pred_probs_adj /= test_pred_probs_adj.sum(axis=1)[:, None]\n",
    "    \n",
    "    # Adjusted MSP\n",
    "    test_adj_msp = 1. - test_pred_probs_adj.max(axis=1)\n",
    "    \n",
    "    # Adjusted Entropy\n",
    "    test_adj_entropy = get_normalized_entropy(test_pred_probs_adj)\n",
    "    \n",
    "    # OOD Object Entropy and MSP (flip the scores to match 0: most cofident)\n",
    "    ood_test_msp = 1. - OOD_msp.score(pred_probs=test_pred_probs)\n",
    "    ood_test_entropy = 1. - OOD_entropy.score(pred_probs=test_pred_probs)\n",
    "    \n",
    "    #### Evaluate scores\n",
    "    \n",
    "    auroc_test_one_minus_max_pred_prob = roc_auc_score(ood_mask, test_one_minus_max_pred_prob)\n",
    "    auroc_test_entropy = roc_auc_score(ood_mask, test_entropy)\n",
    "\n",
    "    auroc_test_adj_one_minus_max_pred_prob = roc_auc_score(ood_mask, test_adj_msp)\n",
    "    auroc_test_adj_entropy = roc_auc_score(ood_mask, test_adj_entropy)\n",
    "    \n",
    "    auroc_test_ood_one_minus_max_pred_prob = roc_auc_score(ood_mask, ood_test_msp)\n",
    "    auroc_test_ood_entropy = roc_auc_score(ood_mask, ood_test_entropy)\n",
    "    \n",
    "    results = {\n",
    "        \"in_distribution\": in_dataset,\n",
    "        \"out_of_distribution\": out_dataset,\n",
    "\n",
    "        \"auroc_test_one_minus_max_pred_prob\": auroc_test_one_minus_max_pred_prob,\n",
    "        \"auroc_test_entropy\": auroc_test_entropy,\n",
    "        \n",
    "        \"auroc_test_adj_mst\": auroc_test_adj_one_minus_max_pred_prob,\n",
    "        \"auroc_test_adj_entropy\": auroc_test_adj_entropy,\n",
    "        \n",
    "        \"auroc_test_ood_msp\": auroc_test_ood_one_minus_max_pred_prob,\n",
    "        \"auroc_test_ood_entropy\": auroc_test_ood_entropy,\n",
    "    }\n",
    "    \n",
    "    results_list.append(results)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5b16a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_distribution': 'cifar-10',\n",
       " 'out_of_distribution': 'cifar-100',\n",
       " 'auroc_test_one_minus_max_pred_prob': 0.9681387100000001,\n",
       " 'auroc_test_entropy': 0.9690872850000001,\n",
       " 'auroc_test_adj_mst': 0.9646997949999999,\n",
       " 'auroc_test_adj_entropy': 0.964940855,\n",
       " 'auroc_test_ood_msp': 0.9646997949999999,\n",
       " 'auroc_test_ood_entropy': 0.964940855}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe3ea0-0fc4-47e0-bd18-cdcae835d106",
   "metadata": {},
   "source": [
    "## Put results to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f079c3ea-edd5-49f0-8d4f-42f8d22ee04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36aac17a-316a-4871-b4c9-c574c92f36eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'in_distribution',\n",
    "    'out_of_distribution',\n",
    "    'auroc_test_one_minus_max_pred_prob',\n",
    "    'auroc_test_adj_mst',\n",
    "#     'auroc_test_ood_msp',\n",
    "    'auroc_test_entropy',\n",
    "    'auroc_test_adj_entropy',\n",
    "#     'auroc_test_ood_entropy',\n",
    "]\n",
    "\n",
    "cols_rename_dict = {\n",
    "    'in_distribution': 'In Distribution',\n",
    "    'out_of_distribution': 'Out of Distribution',\n",
    "    'auroc_test_one_minus_max_pred_prob': 'MSP',\n",
    "    'auroc_test_entropy': 'Entropy',\n",
    "    'auroc_test_adj_mst' : 'Adjusted MSP',\n",
    "    'auroc_test_adj_entropy': 'Adjusted Entropy',\n",
    "#     'auroc_test_ood_msp' : 'OutOfDistribution Class MSP',\n",
    "#     'auroc_test_ood_entropy': 'OutOfDistribution Class Entropy',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "339fc4e8-28d7-430b-9cb8-c8c473a66681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>In Distribution</th>\n",
       "      <th>Out of Distribution</th>\n",
       "      <th>MSP</th>\n",
       "      <th>Adjusted MSP</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Adjusted Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cifar-10</td>\n",
       "      <td>cifar-100</td>\n",
       "      <td>0.968139</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>0.969087</td>\n",
       "      <td>0.964941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cifar-100</td>\n",
       "      <td>cifar-10</td>\n",
       "      <td>0.855283</td>\n",
       "      <td>0.857799</td>\n",
       "      <td>0.880888</td>\n",
       "      <td>0.879942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  In Distribution Out of Distribution       MSP  Adjusted MSP   Entropy  \\\n",
       "0        cifar-10           cifar-100  0.968139      0.964700  0.969087   \n",
       "1       cifar-100            cifar-10  0.855283      0.857799  0.880888   \n",
       "\n",
       "   Adjusted Entropy  \n",
       "0          0.964941  \n",
       "1          0.879942  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns before exporting to latex\n",
    "df_results[cols].rename(columns=cols_rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01185bb-622c-4f87-b19f-acc0cd0fd2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write to Latex file\n",
    "with open(f\"unbalanced_{model}_ood_auroc.tex\", \"w\") as tf:\n",
    "    tf.write(df_results[cols].rename(columns=cols_rename_dict).to_latex(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea870ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77088a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
