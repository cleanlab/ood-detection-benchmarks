{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfbac00-f99d-45d8-a621-2671c27c2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"../../\")\n",
    "\n",
    "from autogluon.vision import ImagePredictor, ImageDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from cleanlab.internal.label_quality_utils import get_normalized_entropy\n",
    "\n",
    "from approximate_nearest_neighbors import ApproxNearestNeighbors\n",
    "\n",
    "# pre-trained model\n",
    "from image_feature_extraction.extract_features_from_image_dir import extract_features_from_image_dir\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7895ad-11e9-4454-90cb-c7c33dae84d9",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2db9dfd-303c-43c2-9439-05a994dee909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Getting data for cifar-10\n",
      "Extracting pre-trained embeddings...\n",
      "Function: init_data_loader_from_image_folder, Time: 0.3103651300043566 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.3106159250019118 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 33.73509187999298 Seconds\n",
      "Function: init_data_loader_from_image_folder, Time: 0.06405507599993143 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.06429411900171544 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 5.223168789001647 Seconds\n",
      "--------------------------\n",
      "Getting data for cifar-100\n",
      "Extracting pre-trained embeddings...\n",
      "Function: init_data_loader_from_image_folder, Time: 0.3105279080045875 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.3107110170094529 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 24.557706528008566 Seconds\n",
      "Function: init_data_loader_from_image_folder, Time: 0.0652280200010864 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.06541212699085008 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 5.158221752993995 Seconds\n",
      "--------------------------\n",
      "Getting data for roman-numeral\n",
      "Extracting pre-trained embeddings...\n",
      "Function: init_data_loader_from_image_folder, Time: 0.015309087990317494 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.015499098997679539 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 2.887093443001504 Seconds\n",
      "Function: init_data_loader_from_image_folder, Time: 0.017234450002433732 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.0174228139949264 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 3.3772088080004323 Seconds\n",
      "--------------------------\n",
      "Getting data for mnist\n",
      "Extracting pre-trained embeddings...\n",
      "Function: init_data_loader_from_image_folder, Time: 0.3762280389928492 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.37642371299443766 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 28.63511177401233 Seconds\n",
      "Function: init_data_loader_from_image_folder, Time: 0.06318879500031471 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.06337376199371647 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 4.988341887990828 Seconds\n",
      "--------------------------\n",
      "Getting data for fashion-mnist\n",
      "Extracting pre-trained embeddings...\n",
      "Function: init_data_loader_from_image_folder, Time: 0.3755347330006771 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.37571845900674816 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 28.908391689998098 Seconds\n",
      "Function: init_data_loader_from_image_folder, Time: 0.08133228099904954 Seconds\n",
      "Function: read_images_as_data_loader, Time: 0.08156460299505852 Seconds\n",
      "Starting ONNX runtime engine...\n",
      "  ONNX runtime device: GPU\n",
      "  ONNX runtime session providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Extracting features...\n",
      "Function: extract_features_from_image_dir, Time: 5.193756565000513 Seconds\n"
     ]
    }
   ],
   "source": [
    "model = \"swin_base_patch4_window7_224\" \n",
    "\n",
    "data_model_dict = {\n",
    "    \"cifar-10\": {\"data_path\": \"/Data/cifar10_png/\"},\n",
    "    \"cifar-100\": {\"data_path\": \"/Data/cifar100_png/\"},\n",
    "    \"roman-numeral\": {\"data_path\": \"/Data/andrew-ng-dcai-comp-2021-data-deduped/andrew-ng-dcai-comp-2021-data/\"},\n",
    "    \"mnist\": {\"data_path\": \"/Data/mnist_png/mnist_png/\"},\n",
    "    \"fashion-mnist\": {\"data_path\": \"/Data/fashion_mnist_png/\"}\n",
    "}\n",
    "\n",
    "# path to pre-trained model in ONNX format\n",
    "path_to_onnx = \"../../image_feature_extraction/models/feature_extractor.onnx\"\n",
    "\n",
    "# Get data, model, and pre-trained features\n",
    "for dataset in data_model_dict.keys():\n",
    "    \n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Getting data for {dataset}\")\n",
    "    \n",
    "    # Get path to data\n",
    "    data_path = data_model_dict[dataset][\"data_path\"]\n",
    "    \n",
    "    # Get train and test data\n",
    "    data_model_dict[dataset][\"train_data\"], _, data_model_dict[dataset][\"test_data\"] = \\\n",
    "        ImageDataset.from_folders(root=data_path)\n",
    "    \n",
    "    # Get path to saved model\n",
    "    data_model_dict[dataset][\"model\"] = f\"./autogluon_models/{model}_{dataset}.ag\"\n",
    "    \n",
    "    # Get pre-trained features; ResNet50 model pre-trained on ImageNet\n",
    "    # NOTE: we will compare these with LEARNED embeddings from the trained model (see below)\n",
    "    print(f\"Extracting pre-trained embeddings...\")\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        data_model_dict[dataset][f\"{split}_data_pretrained_features\"], _, _ = \\\n",
    "            extract_features_from_image_dir(path_to_onnx=path_to_onnx, path_to_image_dir=f\"{data_path}{split}/\", batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0446d-8978-449f-b8c5-0cbdd97d46be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d426f01d-d145-4d18-a265-0b8c866114db",
   "metadata": {},
   "source": [
    "## Evaluate models on test data as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b533948d-5031-4de5-bb1c-0b0ce9eb259a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cifar-10\n",
      "  Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 24] validation: top1=0.988700 top5=0.999900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation: {'loss': 0.11967918761968613, 'top1': 0.9887, 'top5': 0.9999}\n",
      "Dataset: cifar-100\n",
      "  Loading model...\n",
      "  Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 17] validation: top1=0.928600 top5=0.993300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation: {'loss': 0.33964193670749665, 'top1': 0.9286, 'top5': 0.9933}\n",
      "Dataset: roman-numeral\n",
      "  Loading model...\n",
      "  Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 26] validation: top1=0.796694 top5=0.980579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation: {'loss': 0.6779465489643665, 'top1': 0.7966942148760331, 'top5': 0.9805785123966942}\n",
      "Dataset: mnist\n",
      "  Loading model...\n",
      "  Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 24] validation: top1=0.991400 top5=1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation: {'loss': 0.11047064321041107, 'top1': 0.9914, 'top5': 1.0}\n",
      "Dataset: fashion-mnist\n",
      "  Loading model...\n",
      "  Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 37] validation: top1=0.948900 top5=0.999700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation: {'loss': 0.2166707287788391, 'top1': 0.9489, 'top5': 0.9997}\n"
     ]
    }
   ],
   "source": [
    "accuracy_result_list = []\n",
    "\n",
    "for key, val in data_model_dict.items():\n",
    "    \n",
    "    dataset = key\n",
    "    model_path = val[\"model\"]\n",
    "    test_dataset = val[\"test_data\"]\n",
    "    \n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    \n",
    "    # load model\n",
    "    print(\"  Loading model...\")\n",
    "    predictor_loaded = ImagePredictor.load(model_path)\n",
    "    \n",
    "    # evaluating model on test data\n",
    "    print(\"  Evaluating model...\")\n",
    "    eval_ = predictor_loaded.evaluate(test_dataset)\n",
    "    print(f\"    Evaluation: {eval_}\")\n",
    "    \n",
    "    accuracy_result = {\n",
    "        \"dataset\": dataset,\n",
    "        \"top1\": eval_[\"top1\"]\n",
    "    }\n",
    "    \n",
    "    accuracy_result_list.append(accuracy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0ab5c-cb18-42e3-8f7e-4e2f05b81408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d99027a0-8bac-476b-972d-e7a94eaae158",
   "metadata": {},
   "source": [
    "## Evaluate OOD Scores on TEST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4956a7-b3d1-4cb1-8ca4-8e1d0aa0521e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "(in-distribution, out-of-distribution) dataset pair:  cifar-10 cifar-100\n",
      "  Loading model...\n",
      "  Generating predicted probabilities...\n",
      "  Extracting learned embeddings...\n",
      "  Running nearest neighbors search...\n",
      "Building nearest neighbors index\n",
      "  Generating scores...\n",
      "-----------------------------------------------------\n",
      "(in-distribution, out-of-distribution) dataset pair:  cifar-100 cifar-10\n",
      "  Loading model...\n",
      "  Generating predicted probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/autogluon/vision/predictor/predictor.py:548: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  y_pred_proba[list(self._label_cleaner.cat_mappings_dependent_var.values())] = y_pred_proba['image_proba'].to_list()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracting learned embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/autogluon/vision/predictor/predictor.py:548: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  y_pred_proba[list(self._label_cleaner.cat_mappings_dependent_var.values())] = y_pred_proba['image_proba'].to_list()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running nearest neighbors search...\n",
      "Building nearest neighbors index\n",
      "  Generating scores...\n",
      "-----------------------------------------------------\n",
      "(in-distribution, out-of-distribution) dataset pair:  mnist roman-numeral\n",
      "  Loading model...\n",
      "  Generating predicted probabilities...\n",
      "  Extracting learned embeddings...\n",
      "  Running nearest neighbors search...\n",
      "Building nearest neighbors index\n",
      "  Generating scores...\n",
      "-----------------------------------------------------\n",
      "(in-distribution, out-of-distribution) dataset pair:  roman-numeral mnist\n",
      "  Loading model...\n",
      "  Generating predicted probabilities...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# pairs of (in-distribution, out-of-distribution) datasets to evaluate\n",
    "in_out_pairs = [\n",
    "    {\"in\": \"cifar-10\", \"out\": \"cifar-100\"},\n",
    "    {\"in\": \"cifar-100\", \"out\": \"cifar-10\"},\n",
    "    {\"in\": \"mnist\", \"out\": \"roman-numeral\"},\n",
    "    {\"in\": \"roman-numeral\", \"out\": \"mnist\"},\n",
    "    {\"in\": \"mnist\", \"out\": \"fashion-mnist\"},\n",
    "    {\"in\": \"fashion-mnist\", \"out\": \"mnist\"},\n",
    "]\n",
    "\n",
    "k_max = 110 # max k value for K nearest neighbor search\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for in_out_pair in in_out_pairs:\n",
    "    \n",
    "    in_dataset, out_dataset = in_out_pair[\"in\"], in_out_pair[\"out\"]\n",
    "    \n",
    "    # path to model trained on in-distribution train dataset\n",
    "    in_model_path = data_model_dict[in_dataset][\"model\"]\n",
    "\n",
    "    # get TEST datasets used for evaluation\n",
    "    in_test_dataset = data_model_dict[in_dataset][\"test_data\"]\n",
    "    out_test_dataset = data_model_dict[out_dataset][\"test_data\"]\n",
    "    \n",
    "    # class labels for the in-distribution test dataset\n",
    "    in_test_dataset_class_labels = in_test_dataset.label.values    \n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"(in-distribution, out-of-distribution) dataset pair: \", in_dataset, out_dataset)\n",
    "    \n",
    "    # load model\n",
    "    print(\"  Loading model...\")\n",
    "    in_predictor_loaded = ImagePredictor.load(in_model_path)\n",
    "    \n",
    "    # Get test predicted probabilities\n",
    "    print(\"  Generating predicted probabilities...\")\n",
    "    in_pred_probs = in_predictor_loaded.predict_proba(data=in_test_dataset, as_pandas=False)\n",
    "    out_pred_probs = in_predictor_loaded.predict_proba(data=out_test_dataset, as_pandas=False)\n",
    "    \n",
    "    # Get LEARNED embeddings\n",
    "    print(\"  Extracting learned embeddings...\")\n",
    "    in_features = \\\n",
    "        np.stack(\n",
    "            in_predictor_loaded.predict_feature(data=in_test_dataset, as_pandas=False)[:, 0]\n",
    "        )\n",
    "    \n",
    "    out_features = \\\n",
    "        np.stack(\n",
    "            in_predictor_loaded.predict_feature(data=out_test_dataset, as_pandas=False)[:, 0]\n",
    "        )\n",
    "    \n",
    "    # Get pre-trained embeddings\n",
    "    in_pretrained_features = data_model_dict[in_dataset][\"test_data_pretrained_features\"]\n",
    "    out_pretrained_features = data_model_dict[out_dataset][\"test_data_pretrained_features\"]\n",
    "    \n",
    "    # Combine pred_probs and features\n",
    "    pred_probs = np.vstack([in_pred_probs, out_pred_probs])\n",
    "    features = np.vstack([in_features, out_features]) # LEARNED embeddings\n",
    "    pretrained_features = np.vstack([in_pretrained_features, out_pretrained_features])\n",
    "\n",
    "    # Create binary labels (1 = out-of-distribution)\n",
    "    in_labels = np.zeros(shape=len(in_pred_probs))\n",
    "    out_labels = np.ones(shape=len(out_pred_probs))\n",
    "    labels = np.hstack([in_labels, out_labels]) # OOD binary indicator\n",
    "    \n",
    "    print(\"  Running nearest neighbors search...\")\n",
    "    #### Compute nearest neighbors\n",
    "    \n",
    "    # nearest neighbors\n",
    "    nns = ApproxNearestNeighbors(\n",
    "            features=features, # LEARNED embeddings from the trained model\n",
    "            labels=labels,\n",
    "            ) # init Nearest Neighbor Scorer\n",
    "    nns.build_index() # build index for nearest neighbor lookup\n",
    "    neighbors_idx, neighbors_dist, neighbors_labels = nns.get_k_nearest_neighbors(k=k_max)\n",
    "    \n",
    "    print(\"  Generating scores...\")\n",
    "    #### Generate scores\n",
    "    one_minus_max_pred_prob = 1. - pred_probs.max(axis=1)\n",
    "    \n",
    "    entropy = get_normalized_entropy(pred_probs)\n",
    "    get_neighbor_entropy = np.vectorize(lambda idx: entropy[idx]) # Used to get entropy of neighbors\n",
    "    \n",
    "    get_neighbor_pred_probs = np.vectorize(lambda idx: pred_probs[idx], signature='()->(n)') # Used to get pred_probs of neighbors\n",
    "    \n",
    "    knn_k1_dist = neighbors_dist[:, :1].mean(axis=1)\n",
    "    \n",
    "    k = 5\n",
    "    knn_k5_entropy = (get_neighbor_entropy(neighbors_idx[:,:k]).sum(axis=1) + entropy) / (k + 1)\n",
    "    knn_k5_dist = neighbors_dist[:, :k].mean(axis=1)    \n",
    "    \n",
    "    k = 10\n",
    "    knn_k10_entropy = (get_neighbor_entropy(neighbors_idx[:,:k]).sum(axis=1) + entropy) / (k + 1)\n",
    "    knn_k10_dist = neighbors_dist[:, :k].mean(axis=1)\n",
    "    \n",
    "    k = 15\n",
    "    knn_k15_entropy = (get_neighbor_entropy(neighbors_idx[:,:k]).sum(axis=1) + entropy) / (k + 1)    \n",
    "    knn_k15_dist = neighbors_dist[:, :k].mean(axis=1)\n",
    "    \n",
    "    k = 100\n",
    "    knn_k100_entropy = (get_neighbor_entropy(neighbors_idx[:,:k]).sum(axis=1) + entropy) / (k + 1)    \n",
    "    knn_k100_dist = neighbors_dist[:, :k].mean(axis=1)\n",
    "    \n",
    "    # anomaly score (isolation forest)\n",
    "    anomaly_model = IsolationForest(random_state=0, n_estimators=100) # instantiate model\n",
    "    anomaly_model.fit(features) # out-of-sample extracted features\n",
    "    anomaly_score = 1 / anomaly_model.score_samples(features) # take the inverse so higher scores are more anomalous\n",
    "    \n",
    "    # entropy of KNN avg pred_probs\n",
    "    k = 10\n",
    "    neighbors_pred_probs = (get_neighbor_pred_probs(neighbors_idx[:,:k]).sum(axis=1) + pred_probs) / (k + 1)\n",
    "    entropy_knn_k10_pred_probs = get_normalized_entropy(neighbors_pred_probs)\n",
    "    \n",
    "        \n",
    "    #### Evaluate scores\n",
    "    auroc_one_minus_max_pred_prob = roc_auc_score(labels, one_minus_max_pred_prob)\n",
    "    \n",
    "    auroc_entropy = roc_auc_score(labels, entropy)\n",
    "\n",
    "    auroc_knn_k5_entropy = roc_auc_score(labels, knn_k5_entropy)\n",
    "    auroc_knn_k10_entropy = roc_auc_score(labels, knn_k10_entropy)\n",
    "    auroc_knn_k15_entropy = roc_auc_score(labels, knn_k15_entropy)\n",
    "    auroc_knn_k100_entropy = roc_auc_score(labels, knn_k100_entropy)\n",
    "    \n",
    "    auroc_entropy_knn_k10_pred_probs = roc_auc_score(labels, entropy_knn_k10_pred_probs)        \n",
    "\n",
    "    auroc_knn_k1_dist = roc_auc_score(labels, knn_k1_dist)\n",
    "    auroc_knn_k5_dist = roc_auc_score(labels, knn_k5_dist)\n",
    "    auroc_knn_k10_dist = roc_auc_score(labels, knn_k10_dist)\n",
    "    auroc_knn_k15_dist = roc_auc_score(labels, knn_k15_dist)    \n",
    "    auroc_knn_k100_dist = roc_auc_score(labels, knn_k100_dist)        \n",
    "\n",
    "    auroc_anomaly_score = roc_auc_score(labels, anomaly_score)\n",
    "    \n",
    "    results = {\n",
    "        \"in_distribution\": in_dataset,\n",
    "        \"out_of_distribution\": out_dataset,\n",
    "        \"auroc_one_minus_max_pred_prob\": auroc_one_minus_max_pred_prob,\n",
    "        \"auroc_entropy\": auroc_entropy,\n",
    "        \n",
    "        \"auroc_knn_k5_entropy\": auroc_knn_k5_entropy,\n",
    "        \"auroc_knn_k10_entropy\": auroc_knn_k10_entropy,\n",
    "        \"auroc_knn_k15_entropy\": auroc_knn_k15_entropy,\n",
    "        \"auroc_knn_k100_entropy\": auroc_knn_k100_entropy,\n",
    "        \n",
    "        \"auroc_entropy_knn_k10_pred_probs\": auroc_entropy_knn_k10_pred_probs,\n",
    "\n",
    "        \"auroc_knn_k1_dist\": auroc_knn_k1_dist,\n",
    "        \"auroc_knn_k5_dist\": auroc_knn_k5_dist,\n",
    "        \"auroc_knn_k10_dist\": auroc_knn_k10_dist,\n",
    "        \"auroc_knn_k15_dist\": auroc_knn_k15_dist,\n",
    "        \"auroc_knn_k100_dist\": auroc_knn_k100_dist,\n",
    "        \n",
    "        \"auroc_anomaly_score\": auroc_anomaly_score\n",
    "    }\n",
    "    \n",
    "    results_list.append(results)\n",
    "    \n",
    "    \n",
    "    #### Save pred_probs, embeddings, and OOD mask to Numpy files\n",
    "    \n",
    "    # Save files here\n",
    "    out_folder = f\"./test_data_in_{in_dataset}_out_{out_dataset}/\"\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    \n",
    "    np.save(out_folder + \"pred_probs.npy\", pred_probs)\n",
    "    np.save(out_folder + \"embeddings.npy\", features) # features = embeddings extracted from model trained on training dataset\n",
    "    np.save(out_folder + \"ood_mask.npy\", labels) # labels in this context is whether the datapoint is OOD! It's a binary indicator, True = OOD\n",
    "    \n",
    "    np.save(out_folder + \"in_distribution_test_dataset_class_labels.npy\", in_test_dataset_class_labels) # class labels for only the in-distribution test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aeb62f-15c9-4ce8-8c66-7855bad9d0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44a2747-d5e8-43a4-b517-e366dcfcc1c6",
   "metadata": {},
   "source": [
    "## Put results to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04301c44-9d0e-4489-89a5-07f508f5a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2193a03-05e7-4227-a5c5-11cee9bdf84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"in_distribution\",\n",
    "    \"out_of_distribution\",\n",
    "    \"auroc_one_minus_max_pred_prob\",\n",
    "    \"auroc_entropy\",\n",
    "    \"auroc_knn_k10_entropy\",\n",
    "    \"auroc_entropy_knn_k10_pred_probs\",\n",
    "    \"auroc_knn_k10_dist\",\n",
    "    \"auroc_anomaly_score\"\n",
    "]\n",
    "\n",
    "df_results[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750efd28-687d-42f0-9a55-32b7f7ef30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"in_distribution\",\n",
    "    \"out_of_distribution\",\n",
    "    \"auroc_knn_k5_entropy\",\n",
    "    \"auroc_knn_k10_entropy\",\n",
    "    \"auroc_knn_k15_entropy\",\n",
    "    \"auroc_knn_k100_entropy\",\n",
    "]\n",
    "\n",
    "df_results[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e93f8-5058-4e56-b70a-f8ffdbd3f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"in_distribution\",\n",
    "    \"out_of_distribution\",\n",
    "    \"auroc_knn_k1_dist\",\n",
    "    \"auroc_knn_k5_dist\",\n",
    "    \"auroc_knn_k10_dist\",\n",
    "    \"auroc_knn_k15_dist\",\n",
    "    \"auroc_knn_k100_dist\",\n",
    "]\n",
    "\n",
    "df_results[cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
