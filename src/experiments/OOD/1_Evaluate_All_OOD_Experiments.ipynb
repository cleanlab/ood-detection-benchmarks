{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df19a9e-27a8-4869-adc6-c6b9e76f48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"../../\")\n",
    "\n",
    "from autogluon.vision import ImagePredictor, ImageDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from cleanlab.internal.label_quality_utils import get_normalized_entropy\n",
    "\n",
    "from approximate_nearest_neighbors import ApproxNearestNeighbors\n",
    "\n",
    "# mahalanobis\n",
    "from mahalanobis import fit_mahalanobis, score_mahalanobis, fit_rmd, score_rmd\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7895ad-11e9-4454-90cb-c7c33dae84d9",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2db9dfd-303c-43c2-9439-05a994dee909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Getting data for cifar-10\n",
      "--------------------------\n",
      "Getting data for cifar-100\n",
      "--------------------------\n",
      "Getting data for roman-numeral\n",
      "--------------------------\n",
      "Getting data for mnist\n",
      "--------------------------\n",
      "Getting data for fashion-mnist\n",
      "CPU times: user 1.43 s, sys: 99 ms, total: 1.53 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = \"swin_base_patch4_window7_224\" # uses Torch backend\n",
    "# model = \"resnet50_v1\" # uses MXNET backend\n",
    "\n",
    "data_model_dict = {\n",
    "    \"cifar-10\": {\"data_path\": \"/Data/cifar10_png/\"},\n",
    "    \"cifar-100\": {\"data_path\": \"/Data/cifar100_png/\"},\n",
    "    \"roman-numeral\": {\"data_path\": \"/Data/andrew-ng-dcai-comp-2021-data-deduped/andrew-ng-dcai-comp-2021-data/\"},\n",
    "    \"mnist\": {\"data_path\": \"/Data/mnist_png/mnist_png/\"},\n",
    "    \"fashion-mnist\": {\"data_path\": \"/Data/fashion_mnist_png/\"}\n",
    "}\n",
    "\n",
    "# Get data, model, and pre-trained features\n",
    "for dataset in data_model_dict.keys():\n",
    "    \n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Getting data for {dataset}\")\n",
    "    \n",
    "    # Get path to data\n",
    "    data_path = data_model_dict[dataset][\"data_path\"]\n",
    "    \n",
    "    # Get train and test data\n",
    "    data_model_dict[dataset][\"train_data\"], _, data_model_dict[dataset][\"test_data\"] = \\\n",
    "        ImageDataset.from_folders(root=data_path)\n",
    "    \n",
    "    # Get path to saved model\n",
    "    data_model_dict[dataset][\"model\"] = f\"./autogluon_models/{model}_{dataset}.ag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cace151-c2b3-476d-9c4b-0a49f8ca7acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0446d-8978-449f-b8c5-0cbdd97d46be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d426f01d-d145-4d18-a265-0b8c866114db",
   "metadata": {},
   "source": [
    "## Evaluate models on test data as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533948d-5031-4de5-bb1c-0b0ce9eb259a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "accuracy_result_list = []\n",
    "\n",
    "for key, data in data_model_dict.items():\n",
    "    \n",
    "    dataset = key\n",
    "    model_path = data[\"model\"]\n",
    "    test_dataset = data[\"test_data\"]\n",
    "    \n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    \n",
    "    # load model\n",
    "    print(\"  Loading model...\")\n",
    "    predictor_loaded = ImagePredictor.load(model_path)\n",
    "    \n",
    "    # evaluating model on test data\n",
    "    print(\"  Evaluating model...\")\n",
    "    eval_ = predictor_loaded.evaluate(test_dataset)\n",
    "    print(f\"    Evaluation: {eval_}\")\n",
    "    \n",
    "    accuracy_result = {\n",
    "        \"dataset\": dataset,\n",
    "        \"top1\": eval_[\"top1\"]\n",
    "    }\n",
    "    \n",
    "    accuracy_result_list.append(accuracy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0ab5c-cb18-42e3-8f7e-4e2f05b81408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d99027a0-8bac-476b-972d-e7a94eaae158",
   "metadata": {},
   "source": [
    "## Evaluate OOD Scores on TEST data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2932f76-a8c7-4009-bff1-f672da79923a",
   "metadata": {},
   "source": [
    "## Save the pred_probs and features used for OOD scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bea89-2597-4163-9f53-b93090c233b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# pairs of (in-distribution, out-of-distribution) datasets to evaluate\n",
    "in_out_pairs = [\n",
    "    {\"in\": \"cifar-10\", \"out\": \"cifar-100\"},\n",
    "    {\"in\": \"cifar-100\", \"out\": \"cifar-10\"},\n",
    "    {\"in\": \"mnist\", \"out\": \"roman-numeral\"},\n",
    "    {\"in\": \"roman-numeral\", \"out\": \"mnist\"},\n",
    "    {\"in\": \"mnist\", \"out\": \"fashion-mnist\"},\n",
    "    {\"in\": \"fashion-mnist\", \"out\": \"mnist\"},\n",
    "]\n",
    "\n",
    "for in_out_pair in in_out_pairs:\n",
    "    \n",
    "    in_dataset, out_dataset = in_out_pair[\"in\"], in_out_pair[\"out\"]\n",
    "    \n",
    "    # path to model trained on in-distribution train dataset\n",
    "    in_model_path = data_model_dict[in_dataset][\"model\"]\n",
    "\n",
    "    # get in-distribution TRAIN dataset\n",
    "    in_train_dataset = data_model_dict[in_dataset][\"train_data\"]\n",
    "    in_train_dataset_class_labels = in_train_dataset.label.values # class labels for the in-distribution train dataset\n",
    "    \n",
    "    # get TEST datasets used for evaluation\n",
    "    in_test_dataset = data_model_dict[in_dataset][\"test_data\"]\n",
    "    in_test_dataset_class_labels = in_test_dataset.label.values # class labels for the in-distribution test dataset\n",
    "\n",
    "    out_test_dataset = data_model_dict[out_dataset][\"test_data\"]\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"(in-distribution, out-of-distribution) dataset pair: \", in_dataset, out_dataset)\n",
    "    \n",
    "    # load model (trained on training set)\n",
    "    print(\"  Loading model...\")\n",
    "    in_predictor_loaded = ImagePredictor.load(in_model_path)\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    print(\"  Generating predicted probabilities...\")\n",
    "    in_train_pred_probs = in_predictor_loaded.predict_proba(data=in_train_dataset, as_pandas=False)    \n",
    "    in_test_pred_probs = in_predictor_loaded.predict_proba(data=in_test_dataset, as_pandas=False)\n",
    "    out_test_pred_probs = in_predictor_loaded.predict_proba(data=out_test_dataset, as_pandas=False)\n",
    "    \n",
    "    # Get LEARNED embeddings\n",
    "    print(\"  Extracting learned embeddings...\")\n",
    "    in_train_features = \\\n",
    "        np.stack(\n",
    "            in_predictor_loaded.predict_feature(data=in_train_dataset, as_pandas=False)[:, 0]\n",
    "        )\n",
    "    in_test_features = \\\n",
    "        np.stack(\n",
    "            in_predictor_loaded.predict_feature(data=in_test_dataset, as_pandas=False)[:, 0]\n",
    "        )\n",
    "    out_test_features = \\\n",
    "        np.stack(\n",
    "            in_predictor_loaded.predict_feature(data=out_test_dataset, as_pandas=False)[:, 0]\n",
    "        )    \n",
    "    \n",
    "    # Save files here\n",
    "    out_folder = f\"./model_{model}_experiment_in_{in_dataset}_out_{out_dataset}/\"\n",
    "    \n",
    "    # Create folder if it doesn't exist\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    \n",
    "    #### Uncomment below to save files\n",
    "\n",
    "#     np.save(out_folder + \"in_train_pred_probs.npy\", in_train_pred_probs)\n",
    "#     np.save(out_folder + \"in_test_pred_probs.npy\", in_test_pred_probs)\n",
    "#     np.save(out_folder + \"out_test_pred_probs.npy\", out_test_pred_probs)\n",
    "    \n",
    "#     np.save(out_folder + \"in_train_features.npy\", in_train_features)\n",
    "#     np.save(out_folder + \"in_test_features.npy\", in_test_features)\n",
    "#     np.save(out_folder + \"out_test_features.npy\", out_test_features)\n",
    "    \n",
    "#     np.save(out_folder + \"in_train_dataset_class_labels.npy\", in_train_dataset_class_labels)\n",
    "#     np.save(out_folder + \"in_test_dataset_class_labels.npy\", in_test_dataset_class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47837b90-a36f-4c50-a966-33f09b2bb31a",
   "metadata": {},
   "source": [
    "## Run OOD scoring on loaded pred_probs and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18e806-4a90-4662-88ed-bd092593a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p, q):\n",
    "    return -np.sum(p * np.log(q)) / q.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298896e-4a98-4de8-93e9-26dc3e5fd2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# pairs of (in-distribution, out-of-distribution) datasets to evaluate\n",
    "in_out_pairs = [\n",
    "    {\"in\": \"cifar-10\", \"out\": \"cifar-100\"},\n",
    "    {\"in\": \"cifar-100\", \"out\": \"cifar-10\"},\n",
    "    {\"in\": \"mnist\", \"out\": \"roman-numeral\"},\n",
    "    {\"in\": \"roman-numeral\", \"out\": \"mnist\"},\n",
    "    {\"in\": \"mnist\", \"out\": \"fashion-mnist\"},\n",
    "    {\"in\": \"fashion-mnist\", \"out\": \"mnist\"},\n",
    "]\n",
    "\n",
    "k_max = 110\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for in_out_pair in in_out_pairs:\n",
    "    \n",
    "    in_dataset, out_dataset = in_out_pair[\"in\"], in_out_pair[\"out\"]\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"(in-distribution, out-of-distribution) dataset pair: \", in_dataset, out_dataset)\n",
    "    \n",
    "    # Save files here\n",
    "    out_folder = f\"./model_{model}_experiment_in_{in_dataset}_out_{out_dataset}/\"\n",
    "    \n",
    "    # Load files\n",
    "    in_train_pred_probs = np.load(out_folder + \"in_train_pred_probs.npy\")\n",
    "    in_test_pred_probs = np.load(out_folder + \"in_test_pred_probs.npy\")\n",
    "    out_test_pred_probs = np.load(out_folder + \"out_test_pred_probs.npy\")\n",
    "    \n",
    "    in_train_features = np.load(out_folder + \"in_train_features.npy\")\n",
    "    in_test_features = np.load(out_folder + \"in_test_features.npy\", )\n",
    "    out_test_features = np.load(out_folder + \"out_test_features.npy\")\n",
    "    \n",
    "    in_train_dataset_class_labels = np.load(out_folder + \"in_train_dataset_class_labels.npy\")\n",
    "    in_test_dataset_class_labels = np.load(out_folder + \"in_test_dataset_class_labels.npy\")\n",
    "\n",
    "    # Combine pred_probs and features for TEST dataset\n",
    "    test_pred_probs = np.vstack([in_test_pred_probs, out_test_pred_probs])\n",
    "    test_features = np.vstack([in_test_features, out_test_features]) # LEARNED embeddings\n",
    "    \n",
    "    # Create OOD binary labels (1 = out-of-distribution)\n",
    "    in_labels = np.zeros(shape=len(in_test_pred_probs))\n",
    "    out_labels = np.ones(shape=len(out_test_pred_probs))\n",
    "    ood_mask = np.hstack([in_labels, out_labels]).astype(int) # OOD binary indicator\n",
    "\n",
    "    #### Compute nearest neighbors\n",
    "    \n",
    "    # nearest neighbors\n",
    "    nns = ApproxNearestNeighbors(\n",
    "            features=in_train_features, # in-distribution TRAIN dataset features\n",
    "            labels=in_train_dataset_class_labels,\n",
    "            ) # init Nearest Neighbor Scorer\n",
    "    nns.build_index() # build index for nearest neighbor lookup\n",
    "    # _, _, _ = nns.get_k_nearest_neighbors(k=k_max)\n",
    "    \n",
    "    #### Get scores\n",
    "    \n",
    "    # Train Entropy\n",
    "    in_train_entropy = get_normalized_entropy(in_train_pred_probs)\n",
    "    \n",
    "    # Fit Mahalanobis\n",
    "    print(\"Fitting Mahalanobis...\")\n",
    "    num_class = np.unique(in_train_dataset_class_labels).shape[0]\n",
    "    m = fit_mahalanobis(features=in_train_features, labels=in_train_dataset_class_labels, num_class=num_class) # fit on TRAIN in-distribution data\n",
    "    rmd = fit_rmd(features=in_train_features, labels=in_train_dataset_class_labels, num_class=num_class) # fit on TRAIN in-distribution data, Relative Mahalanobis Distance (RMD)\n",
    "    \n",
    "    # Fit Isolation Forest\n",
    "    print(\"Fitting Isolation Forest...\")\n",
    "    if_model = IsolationForest(random_state=0, n_estimators=100) # instantiate model\n",
    "    if_model.fit(in_train_features) # fit on TRAIN in-distribution data\n",
    "    \n",
    "    #### Define vectorized lambda functions to get nearest neighbor scores\n",
    "\n",
    "    # neighbors here are from training datapoints\n",
    "    get_train_neighbor_pred_probs = np.vectorize(lambda idx: in_train_pred_probs[idx], signature='()->(n)') # Used to get pred_probs of neighbors    \n",
    "    get_train_neighbor_entropy = np.vectorize(lambda idx: in_train_entropy[idx]) # Used to get entropy of neighbors\n",
    "    \n",
    "    #### Get nearest neighbors for each test datapoint\n",
    "    neighbors_idx = []\n",
    "    neighbors_dist = []\n",
    "    for v in test_features:\n",
    "        # NOTE: here, the nearest neighbors are from the training dataset\n",
    "        train_idx, train_dist = nns.ann_index.get_nns_by_vector(v, k_max, search_k=-1, include_distances=True)\n",
    "        neighbors_idx.append(train_idx)\n",
    "        neighbors_dist.append(train_dist)\n",
    "    neighbors_idx = np.array(neighbors_idx)\n",
    "    neighbors_dist = np.array(neighbors_dist)\n",
    "    \n",
    "    \n",
    "    #### Get nearest neighbors for each test datapoint with features concatenated with pred_probs\n",
    "    # nearest neighbors with features + pred_probs concatenated\n",
    "    nns_w_pred_probs = ApproxNearestNeighbors(\n",
    "        features=np.hstack([in_train_features, in_train_pred_probs]), # in-distribution TRAIN dataset features + pred_probs concatenated\n",
    "        labels=in_train_dataset_class_labels,\n",
    "    ) # init Nearest Neighbor Scorer\n",
    "    nns_w_pred_probs.build_index() # build index for nearest neighbor lookup\n",
    "    # _, _, _ = nns_w_pred_probs.get_k_nearest_neighbors(k=k_max)\n",
    "\n",
    "    #### Get nearest neighbors for each test datapoint with pred_probs concatenated\n",
    "    test_features_w_pred_probs = np.hstack([test_features, test_pred_probs])\n",
    "    \n",
    "    neighbors_idx_w_pred_probs = []\n",
    "    neighbors_dist_w_pred_probs = []\n",
    "    for v in test_features_w_pred_probs:\n",
    "        # NOTE: here, the nearest neighbors are from the training dataset\n",
    "        train_idx, train_dist = nns_w_pred_probs.ann_index.get_nns_by_vector(v, k_max, search_k=-1, include_distances=True)\n",
    "        neighbors_idx_w_pred_probs.append(train_idx)\n",
    "        neighbors_dist_w_pred_probs.append(train_dist)\n",
    "    neighbors_idx_w_pred_probs = np.array(neighbors_idx_w_pred_probs)\n",
    "    neighbors_dist_w_pred_probs = np.array(neighbors_dist_w_pred_probs)\n",
    "    \n",
    "    \n",
    "    #### Get scores for test dataset\n",
    "    \n",
    "    # 1 - Max Pred Probs\n",
    "    test_one_minus_max_pred_prob = 1. - test_pred_probs.max(axis=1)\n",
    "\n",
    "    # Entropy\n",
    "    test_entropy = get_normalized_entropy(test_pred_probs)\n",
    "\n",
    "    #### KNN scores\n",
    "    \n",
    "    knn_scores_dict = {}\n",
    "    \n",
    "    k_values = [5, 10, 15, 100]\n",
    "    \n",
    "    for k in k_values:\n",
    "    \n",
    "        # KNN entropy\n",
    "        knn_scores_dict[f\"knn_k{str(k)}_test_entropy\"] = (get_train_neighbor_entropy(neighbors_idx[:, :k]).sum(axis=1) + test_entropy) / (k + 1) # include the entropy of test datapoint itself in the avg\n",
    "\n",
    "        # KNN distance\n",
    "        knn_scores_dict[f\"knn_k{str(k)}_test_dist\"] = neighbors_dist[:, :k].mean(axis=1)\n",
    "\n",
    "        # KNN distance where features concatenated with pred_probs\n",
    "        knn_scores_dict[f\"knn_k{str(k)}_test_dist_w_pred_probs\"] = neighbors_dist_w_pred_probs[:, :k].mean(axis=1)\n",
    "\n",
    "        # cross-entropy between test pred_prob and KNN avg pred_probs\n",
    "        neighbor_pred_probs = get_train_neighbor_pred_probs(neighbors_idx[:, :k])\n",
    "        neighbor_pred_probs_avg = neighbor_pred_probs.mean(axis=1)\n",
    "        knn_scores_dict[f\"ce_knn_k{str(k)}_neighbor_pred_probs_avg\"] = -(test_pred_probs * np.log(neighbor_pred_probs_avg)).sum(axis=1) / test_pred_probs.shape[1] # -sum(p * log(q)) / num_classes\n",
    "\n",
    "    \n",
    "    # Mahalanobis\n",
    "    test_mahalanobis_score = score_mahalanobis(test_features, m)\n",
    "    test_relative_mahalanobis_score = score_rmd(test_features, rmd)\n",
    "    \n",
    "    # Isolation Forest\n",
    "    test_isolation_forest_score = 1 / if_model.score_samples(test_features) # take the inverse so higher scores are more anomalous    \n",
    "    \n",
    "    #### Evaluate scores\n",
    "    \n",
    "    auroc_test_one_minus_max_pred_prob = roc_auc_score(ood_mask, test_one_minus_max_pred_prob)\n",
    "    auroc_test_entropy = roc_auc_score(ood_mask, test_entropy)\n",
    "    auroc_test_mahalanobis_score = roc_auc_score(ood_mask, -test_mahalanobis_score) # take the negative so higher scores are more OOD\n",
    "    auroc_test_relative_mahalanobis_score = roc_auc_score(ood_mask, -test_relative_mahalanobis_score) # take the negative so higher scores are more OOD\n",
    "    auroc_test_isolation_forest_score = roc_auc_score(ood_mask, test_isolation_forest_score)\n",
    "    \n",
    "    results = {\n",
    "        \"in_distribution\": in_dataset,\n",
    "        \"out_of_distribution\": out_dataset,\n",
    "\n",
    "        \"auroc_test_one_minus_max_pred_prob\": auroc_test_one_minus_max_pred_prob,\n",
    "        \"auroc_test_entropy\": auroc_test_entropy,\n",
    "        \n",
    "        \"auroc_test_mahalanobis_score\": auroc_test_mahalanobis_score,\n",
    "        \"auroc_test_relative_mahalanobis_score\": auroc_test_relative_mahalanobis_score,        \n",
    "        \"auroc_test_isolation_forest_score\": auroc_test_isolation_forest_score,\n",
    "    }\n",
    "    \n",
    "    # knn scores\n",
    "    for k in k_values:\n",
    "        results[f\"auroc_knn_k{str(k)}_test_entropy\"] = roc_auc_score(ood_mask, knn_scores_dict[f\"knn_k{str(k)}_test_entropy\"])\n",
    "        results[f\"auroc_knn_k{str(k)}_test_dist\"] = roc_auc_score(ood_mask, knn_scores_dict[f\"knn_k{str(k)}_test_dist\"])\n",
    "        results[f\"auroc_knn_k{str(k)}_test_dist_w_pred_probs\"] = roc_auc_score(ood_mask, knn_scores_dict[f\"knn_k{str(k)}_test_dist_w_pred_probs\"])\n",
    "        results[f\"auroc_test_ce_knn_k{str(k)}_neighbor_pred_probs_avg\"] = roc_auc_score(ood_mask, knn_scores_dict[f\"ce_knn_k{str(k)}_neighbor_pred_probs_avg\"])\n",
    "    \n",
    "    results_list.append(results)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5629c-f5c9-4d9c-9966-7522e36ba6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864480b-9fbc-4d65-a1ae-65090a144ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe3ea0-0fc4-47e0-bd18-cdcae835d106",
   "metadata": {},
   "source": [
    "## Put results to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079c3ea-edd5-49f0-8d4f-42f8d22ee04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b036bac-7fa3-4620-ba3e-022fbc9853fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dc95e-b8b1-4775-8bfe-e5dfc46b831b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5cee54-513d-4648-b89d-08e8c0e1f2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aac17a-316a-4871-b4c9-c574c92f36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'in_distribution',\n",
    "    'out_of_distribution',\n",
    "    'auroc_test_one_minus_max_pred_prob',\n",
    "    'auroc_test_entropy',\n",
    "    'auroc_test_mahalanobis_score', \n",
    "    'auroc_test_relative_mahalanobis_score',\n",
    "    'auroc_test_isolation_forest_score', \n",
    "    # 'auroc_knn_k10_test_entropy', \n",
    "    'auroc_knn_k10_test_dist',\n",
    "    'auroc_knn_k10_test_dist_w_pred_probs',\n",
    "    'auroc_test_ce_knn_k10_neighbor_pred_probs_avg',\n",
    "]\n",
    "\n",
    "df_results[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6ee75-a0bc-4739-a72c-747616b99510",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_rename_dict = {\n",
    "    'in_distribution': 'In Distribution',\n",
    "    'out_of_distribution': 'Out of Distribution',\n",
    "    'auroc_test_one_minus_max_pred_prob': 'MSP',\n",
    "    'auroc_test_entropy': 'Entropy',\n",
    "    'auroc_test_mahalanobis_score': 'Mahalanobis',\n",
    "    'auroc_test_relative_mahalanobis_score': 'RMD',\n",
    "    'auroc_test_isolation_forest_score': 'Isolation Forest',\n",
    "    'auroc_knn_k10_test_dist': 'KNN Distance (K=10)',\n",
    "    'auroc_knn_k10_test_dist_w_pred_probs': 'KNN Distance + Predictions (K=10)',\n",
    "    'auroc_test_ce_knn_k10_neighbor_pred_probs_avg': 'KNN Predictions (K=10)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fc4e8-28d7-430b-9cb8-c8c473a66681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns before exporting to latex\n",
    "df_results[cols].rename(columns=cols_rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01185bb-622c-4f87-b19f-acc0cd0fd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to latex\n",
    "\n",
    "# write to Latex file\n",
    "with open(f\"{model}_ood_auroc.tex\", \"w\") as tf:\n",
    "    tf.write(df_results[cols].rename(columns=cols_rename_dict).to_latex(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc39ee-28b8-4666-b943-2807e8268e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927b526-5b7e-40fb-9dd0-b0363b6cca65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1bc78-b893-442e-a8b5-7fcf0436494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cols = [\n",
    "    'in_distribution', \n",
    "    'out_of_distribution',   \n",
    "    \n",
    "#     'auroc_knn_k5_test_entropy',\n",
    "#     'auroc_knn_k10_test_entropy',    \n",
    "#     'auroc_knn_k15_test_entropy',    \n",
    "#     'auroc_knn_k100_test_entropy', \n",
    "    \n",
    "    'auroc_knn_k5_test_dist', \n",
    "    'auroc_knn_k10_test_dist',    \n",
    "    'auroc_knn_k15_test_dist',    \n",
    "    'auroc_knn_k100_test_dist',    \n",
    "    \n",
    "    'auroc_knn_k5_test_dist_w_pred_probs',\n",
    "    'auroc_knn_k10_test_dist_w_pred_probs',    \n",
    "    'auroc_knn_k15_test_dist_w_pred_probs',    \n",
    "    'auroc_knn_k100_test_dist_w_pred_probs',    \n",
    "    \n",
    "    'auroc_test_ce_knn_k5_neighbor_pred_probs_avg', \n",
    "    'auroc_test_ce_knn_k10_neighbor_pred_probs_avg', \n",
    "    'auroc_test_ce_knn_k15_neighbor_pred_probs_avg',\n",
    "    'auroc_test_ce_knn_k100_neighbor_pred_probs_avg'\n",
    "]\n",
    "\n",
    "df_results[knn_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf059c-51a1-4d2d-9b5f-b4fc03da0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cols_rename_dict = {\n",
    "    'in_distribution': 'In Distribution',\n",
    "    'out_of_distribution': 'Out of Distribution', \n",
    "    \n",
    "    'auroc_knn_k5_test_dist': 'KNN Distance (K=5)',\n",
    "    'auroc_knn_k10_test_dist': 'KNN Distance (K=10)',\n",
    "    'auroc_knn_k15_test_dist': 'KNN Distance (K=15)',\n",
    "    'auroc_knn_k100_test_dist': 'KNN Distance (K=100)',\n",
    "    \n",
    "    'auroc_knn_k5_test_dist_w_pred_probs': 'KNN Distance + Predictions (K=5)',\n",
    "    'auroc_knn_k10_test_dist_w_pred_probs': 'KNN Distance + Predictions (K=10)', \n",
    "    'auroc_knn_k15_test_dist_w_pred_probs': 'KNN Distance + Predictions (K=15)',\n",
    "    'auroc_knn_k100_test_dist_w_pred_probs': 'KNN Distance + Predictions (K=100)',\n",
    "    \n",
    "    'auroc_test_ce_knn_k5_neighbor_pred_probs_avg': 'KNN Predictions (K=5)',\n",
    "    'auroc_test_ce_knn_k10_neighbor_pred_probs_avg': 'KNN Predictions (K=10)',\n",
    "    'auroc_test_ce_knn_k15_neighbor_pred_probs_avg': 'KNN Predictions (K=15)',\n",
    "    'auroc_test_ce_knn_k100_neighbor_pred_probs_avg': 'KNN Predictions (K=100)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758ba30-67d8-4ddf-bec9-9181406c1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns before exporting to latex\n",
    "df_results[knn_cols].rename(columns=knn_cols_rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31827ba4-3c40-472a-bf14-4ae56801264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to latex\n",
    "\n",
    "# write to Latex file\n",
    "with open(f\"{model}_ood_knn_k_values_auroc.tex\", \"w\") as tf:\n",
    "    tf.write(df_results[knn_cols].rename(columns=knn_cols_rename_dict).to_latex(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e976d-6426-4cfe-b625-1394f453c78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8662c663-fcff-4d5c-b2cf-2ac2379b8e7d",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320c23c-c977-461c-af1e-ecf5a358e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#### Choose dataset pair\n",
    "in_dataset = \"cifar-100\"\n",
    "out_dataset = \"cifar-10\"\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"(in-distribution, out-of-distribution) dataset pair: \", in_dataset, out_dataset)\n",
    "\n",
    "# Save files here\n",
    "out_folder = f\"./model_{model}_experiment_in_{in_dataset}_out_{out_dataset}/\"\n",
    "\n",
    "# Load files\n",
    "in_train_pred_probs = np.load(out_folder + \"in_train_pred_probs.npy\")\n",
    "in_test_pred_probs = np.load(out_folder + \"in_test_pred_probs.npy\")\n",
    "out_test_pred_probs = np.load(out_folder + \"out_test_pred_probs.npy\")\n",
    "\n",
    "in_train_features = np.load(out_folder + \"in_train_features.npy\")\n",
    "in_test_features = np.load(out_folder + \"in_test_features.npy\", )\n",
    "out_test_features = np.load(out_folder + \"out_test_features.npy\")\n",
    "\n",
    "in_train_dataset_class_labels = np.load(out_folder + \"in_train_dataset_class_labels.npy\")\n",
    "in_test_dataset_class_labels = np.load(out_folder + \"in_test_dataset_class_labels.npy\")\n",
    "\n",
    "# Combine pred_probs and features for TEST dataset\n",
    "test_pred_probs = np.vstack([in_test_pred_probs, out_test_pred_probs])\n",
    "test_features = np.vstack([in_test_features, out_test_features]) # LEARNED embeddings\n",
    "\n",
    "# Create OOD binary labels (1 = out-of-distribution)\n",
    "in_labels = np.zeros(shape=len(in_test_pred_probs))\n",
    "out_labels = np.ones(shape=len(out_test_pred_probs))\n",
    "ood_mask = np.hstack([in_labels, out_labels]).astype(int) # OOD binary indicator\n",
    "\n",
    "# UMAP projection to 2D\n",
    "reducer = umap.UMAP()\n",
    "umap_embeddings = reducer.fit_transform(test_features) # project test features to 2D    \n",
    "\n",
    "# Generate UMAP plot\n",
    "df_umap = pd.DataFrame({\n",
    "    \"e0\": umap_embeddings[:, 0],\n",
    "    \"e1\": umap_embeddings[:, 1],\n",
    "    \"ood_mask\": ood_mask\n",
    "})\n",
    "\n",
    "df_umap[\"dataset\"] = df_umap.ood_mask.map(lambda idx: (\"in: \" + in_dataset, \"out: \" + out_dataset)[int(idx)])\n",
    "\n",
    "g = sns.scatterplot(data=df_umap, x=\"e0\", y=\"e1\", hue=\"dataset\", s=30)\n",
    "\n",
    "g.legend(fontsize=20)\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(15,10)})\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Change size of marker in legend\n",
    "for lh in g.legend_.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "    lh._sizes = [100] \n",
    "\n",
    "# g.set_title(f\"UMAP 2D Projection of Test Set \\n Embeddings extracted from {model} trained on in-distribution data \\n ---- \\n in-distribution: {in_dataset} \\n out-of-distribution: {out_dataset}\", fontsize=20)\n",
    "g.set_xlabel(\"UMAP Dimension 0\", fontsize=20)\n",
    "g.set_ylabel(\"UMAP Dimension 1\", fontsize=20)\n",
    "\n",
    "g.figure.savefig(f\"{model}_in_{in_dataset}_out_{out_dataset}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82537461-5915-40a7-a988-fcf3e1866cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8ac69-1853-4a9a-a058-66f6d95c99b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8a3ed-280e-4987-971e-0ebab97cb360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938575b-eea3-49ad-a4af-959a80cc6acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
